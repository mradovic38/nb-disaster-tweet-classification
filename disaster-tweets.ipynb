{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "jeqymUTZcDJk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "jWvwI-4PEN51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPr6WcXKeTr4",
        "outputId": "ea5727f3-231d-478b-ae1d-457523b52986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       | ... \n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data"
      ],
      "metadata": {
        "id": "Plzu3UtjER4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zbdx9r3eb2O",
        "outputId": "49c95770-614a-46e4-ac1a-f15639d96175"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "DIR = r'/content/drive/Shareddrives/Materijali 2023 2024/6. semestar/Mašinsko učenje/Vežbe/Domaci 1/ml_d1_x_y_z.zip'\n",
        "\n",
        "with zipfile.ZipFile(DIR, 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "metadata": {
        "id": "pdQKxDtVejjW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = 'ml_d1_x_y_z/data/disaster-tweets.csv'"
      ],
      "metadata": {
        "id": "IOiZfT7HemEF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "X, y = [], []\n",
        "with open(DATA_DIR, 'r') as f:\n",
        "  reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
        "  next(reader, None)\n",
        "  for row in reader:\n",
        "    y.append(int(row[4]))\n",
        "    X.append([row[1], row[2], row[3]])\n",
        "\n",
        "X = np.array(X, dtype=str)\n",
        "y = np.array(y, dtype=np.int32)\n",
        "X[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaS2NcGVfrH-",
        "outputId": "e39e8f9d-6ad1-4b32-a60a-64eba4805ae1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['', '',\n",
              "        'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'],\n",
              "       ['', '', 'Forest fire near La Ronge Sask. Canada'],\n",
              "       ['', '',\n",
              "        \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"],\n",
              "       ['', '',\n",
              "        '13,000 people receive #wildfires evacuation orders in California '],\n",
              "       ['', '',\n",
              "        'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school ']],\n",
              "      dtype='<U157')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing data"
      ],
      "metadata": {
        "id": "w6PmQbKFEXGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputing missing values"
      ],
      "metadata": {
        "id": "LYfyOlylEmiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Text NaN: {np.count_nonzero(X[:, 2]=='')} / {len(X[:,2])}\")\n",
        "print(f\"Keyword NaN: {np.count_nonzero(X[:, 0]=='')} / {len(X[:,0])}\")\n",
        "print(f\"Location NaN: {np.count_nonzero(X[:, 1]=='')} / { len(X[:,1])}\")\n",
        "print(f'Target NaN: {np.isnan(y).sum() } / {len(y)}')\n",
        "print('\\n')\n",
        "\n",
        "countries_not_empty = [i for i in X[:,1] if i]\n",
        "u, indices = np.unique(countries_not_empty, return_inverse=True)\n",
        "most_freq_cntry = u[np.argmax(np.bincount(indices))]\n",
        "print(f'Most common location: {most_freq_cntry}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "372onU2yf5xs",
        "outputId": "d195b114-0c93-4681-98bc-950fb7760843"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text NaN: 0 / 7613\n",
            "Keyword NaN: 61 / 7613\n",
            "Location NaN: 2533 / 7613\n",
            "Target NaN: 0 / 7613\n",
            "\n",
            "\n",
            "Most common location: USA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def imputing(X):\n",
        "  '''\n",
        "  Imputes missing values for keyword and location columns\n",
        "  '''\n",
        "  X[1][X[1] == ''] = 'USA'\n",
        "\n",
        "  def fill_nan(row):\n",
        "    '''\n",
        "    Imputes the missing keyword with the first hashtag if it\n",
        "    exists, otherwise with 'no_keyword'\n",
        "    '''\n",
        "    if row[0]=='':\n",
        "      if '#' in row[2] and not row[2].endswith('#'):\n",
        "        return row[2].split('#')[-1].split()[0]\n",
        "      return 'no_keyword'\n",
        "    else:\n",
        "      return row[0]\n",
        "\n",
        "  X[0] = np.apply_along_axis(fill_nan, axis=0, arr=X)\n",
        "  return X"
      ],
      "metadata": {
        "id": "8De5oOtsq8Ci"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "SY91Luq0EoxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.T # transpose X"
      ],
      "metadata": {
        "id": "lg-SYmpBc7qA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import html\n",
        "import re\n",
        "\n",
        "\n",
        "def remove_urls(text):\n",
        "  '''\n",
        "  Removes URLs, HTML escape characters and special characters\n",
        "  '''\n",
        "  text = html.unescape(text)\n",
        "  re_url = r'https?://\\S+|www\\.\\S+'\n",
        "  re_special = r'[^a-zA-Z0-9\\s]'\n",
        "  text = re.sub(re_url, '', text)\n",
        "  text = re.sub(re_special, ' ', text)\n",
        "  return text.lower()\n",
        "\n",
        "def stem(text):\n",
        "  '''\n",
        "  Stems the words\n",
        "  '''\n",
        "  final = \"\"\n",
        "  for word in text.split():\n",
        "    final += \" \" + porter.stem(word)\n",
        "  return final[1:]\n",
        "\n",
        "def remove_stopword(text):\n",
        "  '''\n",
        "  Remove stop-words\n",
        "  '''\n",
        "  final_text =\"\";\n",
        "  for word in text.split():\n",
        "    if word not in stopwrds:\n",
        "      final_text+= \" \" + word\n",
        "\n",
        "  return final_text[1:]\n",
        "\n",
        "print(X[2])\n",
        "\n",
        "# remove urls\n",
        "remove_urls_vec = np.vectorize(remove_urls)\n",
        "X[2] = remove_urls_vec(X[2])\n",
        "print(X[2])\n",
        "\n",
        "# remove stopwords\n",
        "stopwrds = stopwords.words('english')\n",
        "remove_stopword_vec = np.vectorize(remove_stopword)\n",
        "X[2] = remove_stopword_vec(X[2])\n",
        "print(X[2])\n",
        "\n",
        "# stem\n",
        "porter = PorterStemmer()\n",
        "stem_vec = np.vectorize(stem)\n",
        "X[2] = stem_vec(X[2])\n",
        "print(X[2])\n",
        "\n",
        "# impute missing values\n",
        "X = imputing(X)\n",
        "print(X[1])\n",
        "print(X[0])\n",
        "\n",
        "# tokenize\n",
        "tokenized = [word_tokenize(sentence) for sentence in X[2]]\n",
        "\n",
        "keywords = X[0]\n",
        "locations = X[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJaznYHCh0rZ",
        "outputId": "bf63069f-b78a-449a-d65a-bd4bd9018594"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'\n",
            " 'Forest fire near La Ronge Sask. Canada'\n",
            " \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"\n",
            " ... 'M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ'\n",
            " 'Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.'\n",
            " 'The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d']\n",
            "['our deeds are the reason of this  earthquake may allah forgive us all'\n",
            " 'forest fire near la ronge sask  canada'\n",
            " 'all residents asked to  shelter in place  are being notified by officers  no other evacuation or shelter in place orders are expected'\n",
            " ... 'm1 94  01 04 utc  5km s of volcano hawaii  '\n",
            " 'police investigating after an e bike collided with a car in little portugal  e bike rider suffered serious non life threatening injuries '\n",
            " 'the latest  more homes razed by northern california wildfire   abc news ']\n",
            "['deeds reason earthquake may allah forgive us'\n",
            " 'forest fire near la ronge sask canada'\n",
            " 'residents asked shelter place notified officers evacuation shelter place orders expected'\n",
            " ... 'm1 94 01 04 utc 5km volcano hawaii'\n",
            " 'police investigating e bike collided car little portugal e bike rider suffered serious non life threatening injuries'\n",
            " 'latest homes razed northern california wildfire abc news']\n",
            "['deed reason earthquak may allah forgiv us'\n",
            " 'forest fire near la rong sask canada'\n",
            " 'resid ask shelter place notifi offic evacu shelter place order expect'\n",
            " ... 'm1 94 01 04 utc 5km volcano hawaii'\n",
            " 'polic investig e bike collid car littl portug e bike rider suffer seriou non life threaten injuri'\n",
            " 'latest home raze northern california wildfir abc news']\n",
            "['USA' 'USA' 'USA' ... 'USA' 'USA' 'USA']\n",
            "['no_keyword' 'no_keyword' 'no_keyword' ... 'no_keyword' 'no_keyword'\n",
            " 'no_keyword']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization"
      ],
      "metadata": {
        "id": "vonPe--qFP8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Vocab"
      ],
      "metadata": {
        "id": "GHTNlQYtFUDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Creating the vocab...')\n",
        "vocab_set = set()\n",
        "for doc in tokenized:\n",
        "  for word in doc:\n",
        "    vocab_set.add(word)\n",
        "vocab = list(vocab_set)\n",
        "\n",
        "print('Vocab:', list(zip(vocab, range(len(vocab)))))\n",
        "print('Feature vector size: ', len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I83tquzVFbiG",
        "outputId": "137d1873-6cd8-4df0-96ec-50d4b96fa7d1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating the vocab...\n",
            "Vocab: [('selfavow', 0), ('orch', 1), ('30stm', 2), ('juic1', 3), ('remand', 4), ...]\n",
            "Feature vector size:  13830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Vectorization"
      ],
      "metadata": {
        "id": "2YlBkvmkFtEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def freq_score(word, doc):\n",
        "  return doc.count(word) / len(doc)\n",
        "\n",
        "\n",
        "def tfidf_score(word, doc):\n",
        "  tf = freq_score(word, doc)\n",
        "  idf = idf_table[word]\n",
        "  return tf * idf\n",
        "\n",
        "np.set_printoptions(precision=2, linewidth=200)\n",
        "\n",
        "print('Calculating the IDF table...')\n",
        "doc_counts = dict()\n",
        "for word in vocab:\n",
        "  doc_counts[word] = 0\n",
        "  for doc in tokenized:\n",
        "    if word in doc:\n",
        "      doc_counts[word] += 1\n",
        "print('Doc counts:')\n",
        "print(doc_counts)\n",
        "idf_table = dict()\n",
        "for word in vocab:\n",
        "  idf = math.log10(len(tokenized)/doc_counts[word])\n",
        "  idf_table[word] = idf\n",
        "print('IDF table:')\n",
        "print(idf_table)\n",
        "\n",
        "\n",
        "print('Creating TF-IDF features...')\n",
        "X = np.zeros((len(tokenized), len(vocab)), dtype=np.float32)\n",
        "for doc_idx in range(len(tokenized)):\n",
        "  doc = tokenized[doc_idx]\n",
        "  for word_idx in range(len(vocab)):\n",
        "    word = vocab[word_idx]\n",
        "    cnt = tfidf_score(word, doc)\n",
        "    X[doc_idx][word_idx] = cnt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9szEyj_yJwl",
        "outputId": "ef667231-b7d1-49d3-bda6-d4f9ecaeee7e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating the IDF table...\n",
            "Doc counts:\n",
            "{'selfavow': 1, 'orch': 1, '30stm': 1, 'juic1': 1, 'remand': 1, ...}\n",
            "IDF table:\n",
            "{'selfavow': 3.8815558297933115, 'orch': 3.8815558297933115, '30stm': 3.8815558297933115, 'juic1': 3.8815558297933115, 'remand': 3.8815558297933115, ...}\n",
            "Creating TF-IDF features...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "1aZecBLGqBKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialNaiveBayes:\n",
        "  def __init__(self, nb_classes, nb_words, pseudocount):\n",
        "    self.nb_classes = nb_classes # number of classes\n",
        "    self.nb_words = nb_words # number of words in vocabulary in our case\n",
        "    self.pseudocount = pseudocount # alpha parameter (smoothing)\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    '''\n",
        "    Fits X and y on the model\n",
        "    '''\n",
        "\n",
        "    nb_examples = X.shape[0] # number of training examples\n",
        "\n",
        "    # Calculating P(class) - priors\n",
        "    # np.bincount returns the number of appearances of each number in the interval [0, max_number_in_the_list] in the list\n",
        "    self.priors = np.bincount(Y) / nb_examples # P(class) = number_of_appearances / number_of_examples\n",
        "    print('Priors:')\n",
        "    print(self.priors)\n",
        "\n",
        "    # Calculates the number of appearances of each word for each class\n",
        "    occs = np.zeros((self.nb_classes, self.nb_words))\n",
        "    for i in range(nb_examples):\n",
        "      c = Y[i] # current example class\n",
        "      for w in range(self.nb_words):\n",
        "        cnt = X[i][w] # number of appearances of the word in the features\n",
        "        occs[c][w] += cnt # number of appearances of the word for the class\n",
        "    print('Occurences:')\n",
        "    print(occs)\n",
        "\n",
        "    # Calculating P(word_i | class) - likelihoods\n",
        "    self.like = np.zeros((self.nb_classes, self.nb_words))\n",
        "    for c in range(self.nb_classes):\n",
        "      for w in range(self.nb_words):\n",
        "        up = occs[c][w] + self.pseudocount # number of appearances of the word + alpha\n",
        "        down = np.sum(occs[c]) + self.nb_words*self.pseudocount # total number of words in a class + total number of words (vocab size) * alpha\n",
        "        self.like[c][w] = up / down # likelihood\n",
        "    print('Likelihoods:')\n",
        "    print(self.like)\n",
        "\n",
        "  def predict(self, example):\n",
        "    '''\n",
        "    Predicts the class of the example (with log)\n",
        "    '''\n",
        "    # Calculating P(class | example) for each class\n",
        "    probs = np.zeros(self.nb_classes)\n",
        "    for c in range(self.nb_classes):\n",
        "      prob = np.log(self.priors[c]) # using log for lower chances of overwlof\n",
        "      for w in range(self.nb_words):\n",
        "        cnt = example[w]\n",
        "        prob += cnt * np.log(self.like[c][w]) # addition instead of multiplication, and log\n",
        "      probs[c] = prob\n",
        "\n",
        "    #print('\\\"Probabilites\\\" for a test (with log):')\n",
        "    #print(probs)\n",
        "\n",
        "    # Finding the class with the largest probability\n",
        "    prediction = np.argmax(probs)\n",
        "    return prediction\n",
        "\n",
        "  def predict_multiply(self, example):\n",
        "    '''\n",
        "    Predicts the class of the example (without log)\n",
        "    '''\n",
        "    # Calculating P(class | example) for each class\n",
        "    probs = np.zeros(self.nb_classes)\n",
        "    for c in range(self.nb_classes):\n",
        "      prob = self.priors[c] # no log\n",
        "      for w in range(self.nb_words):\n",
        "        cnt = example[w]\n",
        "        prob *= self.like[c][w] ** cnt # multiplying and scaling\n",
        "      probs[c] = prob\n",
        "\n",
        "    #print('\\\"Probabilites\\\" for a test (without log):')\n",
        "    #print(probs)\n",
        "\n",
        "    # Finding the class with the largest probability\n",
        "    prediction = np.argmax(probs)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "N0wbC7EqhA54"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score(model, X_test, y_test):\n",
        "  preds = []\n",
        "  for i in range(X_test.shape[0]):\n",
        "    preds.append(model.predict(X_test[i]))\n",
        "\n",
        "  accuracy = (preds==y_test).mean()\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "v8FvBdbu4Ija"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the mean accuracy between three consecutive runs"
      ],
      "metadata": {
        "id": "thqJBVG8wuBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = []\n",
        "\n",
        "for i in range(3):\n",
        "  # nasumicno promesaj\n",
        "  n_samples = X.shape[0]\n",
        "  np.random.seed(i) # random seed za reproduktabilnost\n",
        "  indices = np.random.permutation(n_samples)\n",
        "  X_shuff = X[indices]\n",
        "  y_shuff = y[indices]\n",
        "\n",
        "  # train-test split\n",
        "  n_train_samples = int(0.8 * n_samples)\n",
        "  X_train = X_shuff[:n_train_samples][:]\n",
        "  X_test = X_shuff[n_train_samples:][:]\n",
        "  y_train = y_shuff[:n_train_samples]\n",
        "  y_test = y_shuff[n_train_samples:]\n",
        "\n",
        "  model = MultinomialNaiveBayes(nb_classes=2, nb_words=len(vocab), pseudocount=1)\n",
        "  model.fit(X_train, y_train)\n",
        "  accuracies.append(score(model, X_test, y_test))\n",
        "\n",
        "print(f'Mean accuracy: {sum(accuracies) / len(accuracies)}')"
      ],
      "metadata": {
        "id": "g985KZqMnPSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b106c25e-9602-4631-d839-7ffa125edf75"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priors:\n",
            "[0.57 0.43]\n",
            "Occurences:\n",
            "[[0.3  0.   0.   ... 0.23 1.61 1.93]\n",
            " [0.   0.3  0.   ... 0.   0.81 0.  ]]\n",
            "Likelihoods:\n",
            "[[5.62e-05 4.33e-05 4.33e-05 ... 5.31e-05 1.13e-04 1.27e-04]\n",
            " [4.88e-05 6.33e-05 4.88e-05 ... 4.88e-05 8.84e-05 4.88e-05]]\n",
            "Priors:\n",
            "[0.57 0.43]\n",
            "Occurences:\n",
            "[[0.   0.   0.   ... 0.23 2.41 2.44]\n",
            " [0.   0.3  0.97 ... 0.   1.07 0.  ]]\n",
            "Likelihoods:\n",
            "[[4.32e-05 4.32e-05 4.32e-05 ... 5.31e-05 1.48e-04 1.49e-04]\n",
            " [4.88e-05 6.34e-05 9.62e-05 ... 4.88e-05 1.01e-04 4.88e-05]]\n",
            "Priors:\n",
            "[0.57 0.43]\n",
            "Occurences:\n",
            "[[0.   0.   0.   ... 0.23 1.72 1.84]\n",
            " [0.   0.3  0.97 ... 0.   1.07 0.  ]]\n",
            "Likelihoods:\n",
            "[[4.33e-05 4.33e-05 4.33e-05 ... 5.32e-05 1.18e-04 1.23e-04]\n",
            " [4.87e-05 6.33e-05 9.60e-05 ... 4.87e-05 1.01e-04 4.87e-05]]\n",
            "Mean accuracy: 0.7957977675640183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Validation (not used)"
      ],
      "metadata": {
        "id": "jeqymUTZcDJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#def kfold_indices(X, k):\n",
        "    #fold_size = len(X) // k\n",
        "    #indices = np.arange(len(X))\n",
        "    #folds = []\n",
        "   # for i in range(k):\n",
        "      #  test_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
        "      #  train_indices = np.concatenate([indices[:i * fold_size], indices[(i + 1) * fold_size:]])\n",
        "      #  folds.append((train_indices, test_indices))\n",
        "    #return folds\n",
        "\n",
        "#fold_indices = kfold_indices(X, 3)\n"
      ],
      "metadata": {
        "id": "d6O2q6OY3Bwr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for train_indices, test_indices in fold_indices:\n",
        "  #scores=[]\n",
        "  #X_train, y_train = X[train_indices], y[train_indices]\n",
        "  #X_test, y_test = X[test_indices], y[test_indices]\n",
        "\n",
        "  # Train the model on the training data\n",
        "  #model = MultinomialNaiveBayes(nb_classes=2, nb_words=len(vocab), pseudocount=1)\n",
        "  #model.fit(X_train, y_train)\n",
        "\n",
        "  # Calculate the accuracy score for this fold\n",
        "  #fold_score = score(X_test, y_test)\n",
        "\n",
        "  # Append the fold score to the list of scores\n",
        "  #scores.append(fold_score)\n",
        "\n",
        "#mean_accuracy = np.mean(scores)"
      ],
      "metadata": {
        "id": "N0GraFYP3UXo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##mean_accuracy"
      ],
      "metadata": {
        "id": "qmnWU7f67vNx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion"
      ],
      "metadata": {
        "id": "w_Ov_EE188PA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert tokenized sequence to numpy array"
      ],
      "metadata": {
        "id": "O37sDdObcqO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the maximum length of tokenized sentences\n",
        "max_len = max(len(tokens) for tokens in tokenized)\n",
        "\n",
        "# Create an empty array with the appropriate shape\n",
        "tokenized_arr = np.empty((len(tokenized), max_len), dtype=object)\n",
        "\n",
        "# Fill the array with tokenized sentences\n",
        "for i, tokens in enumerate(tokenized):\n",
        "    tokenized_arr[i, :len(tokens)] = tokens\n",
        "\n",
        "#tokenized_arr = tokenized_arr[indices]\n",
        "\n",
        "tokenized_arr = np.where(tokenized_arr == None, \"\", tokenized_arr)"
      ],
      "metadata": {
        "id": "cxTqQXdNIvtx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top 5 most frequent words in each class"
      ],
      "metadata": {
        "id": "Ba6ZgYkXc4Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zeros_indices = [i for i, value in enumerate(y) if value == 0]\n",
        "ones_indices = [i for i, value in enumerate(y) if value == 1]\n",
        "\n",
        "unique_elements_zeros, counts_zeros = np.unique(tokenized_arr[zeros_indices].flatten(), return_counts=True)\n",
        "unique_elements_ones, counts_ones = np.unique(tokenized_arr[ones_indices].flatten(), return_counts=True)\n",
        "\n",
        "\n",
        "zeros_dict = dict(zip(unique_elements_zeros, counts_zeros))\n",
        "sorted_zeros_dict = dict(sorted(zeros_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "print(sorted_zeros_dict)\n",
        "\n",
        "ones_dict = dict(zip(unique_elements_ones, counts_ones))\n",
        "sorted_ones_dict = dict(sorted(ones_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "print(sorted_ones_dict)\n",
        "\n",
        "top_5_zeros = list(sorted_zeros_dict.keys())[1:6]\n",
        "top_5_ones = list(sorted_ones_dict.keys())[1:6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePFDxQ2NFAy_",
        "outputId": "0c832ffe-6594-4673-d6cb-db288fa41fb9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'': 69696, 'like': 308, 'get': 223, 'new': 171, 'go': 144, 'one': 139, ...}\n",
            "{'': 48832, 'fire': 273, 'bomb': 186, 'kill': 162, 'news': 141, ...}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Top 5 most frequent words in tweets labeled as disasters: {top_5_ones}')\n",
        "print(f'Top 5 most frequent words in tweets labeled as not disasters: {top_5_zeros}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8oJex28MAG6",
        "outputId": "c0900c09-fa91-46c4-bd4a-e410557a30fb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most frequent words in tweets labeled as disasters: ['fire', 'bomb', 'kill', 'news', 'flood']\n",
            "Top 5 most frequent words in tweets labeled as not disasters: ['like', 'get', 'new', 'go', 'one']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 5 most frequent words in tweets with accidents make sense because they indicate potential accidents, and there is also the word news, which is often mentioned because accidents are often mentioned in the news. As for the 5 most common words in tweets that are not accidents, they are some common words in normal speech.\n"
      ],
      "metadata": {
        "id": "0PWFQUX1aReb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different Metric"
      ],
      "metadata": {
        "id": "_KO2znjMaI0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introducing a different metric $$LR(word) = ones(word) / zeros(word)$$\n",
        "Where:\\\n",
        "$ones(word)$ is the number of appearances of the word in tweets labeled as disasters\\\n",
        "$zeros(word)$ is the number of appearances of the word in tweets labeled as not disasters.\\\n",
        "We will test this metric for the words that appear at least 10 times in both of the classes."
      ],
      "metadata": {
        "id": "mkTFLTObz6Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = dict()\n",
        "\n",
        "for key in sorted_ones_dict.keys():\n",
        "  if sorted_ones_dict[key] >= 10:\n",
        "    if key in sorted_zeros_dict.keys() and sorted_zeros_dict[key] >=10:\n",
        "      lr[key] = sorted_ones_dict[key] / sorted_zeros_dict[key]\n",
        "\n",
        "sorted_lr = dict(sorted(lr.items(), key=lambda item: item[1], reverse=True))\n",
        "top_5_lr = list(sorted_lr.keys())[:5]\n",
        "bottom_5_lr = list(sorted_lr.keys())[-5:]"
      ],
      "metadata": {
        "id": "FbT9DxUHSVqx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Top 5 highest LRs: {top_5_lr}')\n",
        "print(f'Top 5 lowest LRs: {bottom_5_lr}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crBgDUIVQ4Ns",
        "outputId": "645b9519-648f-49d7-d78c-c88be75d2f89"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 highest LRs: ['kill', 'warn', 'train', 'report', 'latest']\n",
            "Top 5 lowest LRs: ['let', 'scream', 'obliter', 'love', 'full']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LR metric gave better results. Here, instead of looking at how many times a word appeared, we look at how often/less often it appeared in accident tweets. This is how we got a better understanding of the words that often appear in both accident tweets and ordinary tweets. For example. the low LR value for terms like scream and obliter indicates that they are mentioned much more often in the context of non-accident tweets, the opposite is true for words like kill, warn and train."
      ],
      "metadata": {
        "id": "6i1GRqt5azj9"
      }
    }
  ]
}
